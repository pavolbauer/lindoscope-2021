{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c70ea2c-77b4-4fbf-9b50-7f20a14f32c4",
   "metadata": {},
   "source": [
    "This notebook provides some examples of how to process high-resolution videos of a mouse face using DeepLabCut output.\n",
    "\n",
    "Author: Oliver Barnstedt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82c553-cf06-4a7e-a1ea-ca77d37db9fe",
   "metadata": {},
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253023d-8a28-4248-be1e-37fd8bd76c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# imports\n",
    "import pandas as pd\n",
    "try:\n",
    "    import pandas_bokeh\n",
    "except:\n",
    "    !pip install pandas-bokeh\n",
    "    import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "try:\n",
    "    import flow_vis  # visualisation from https://github.com/tomrunia/OpticalFlow_Visualization\n",
    "except:\n",
    "    !pip install flow_vis\n",
    "    import flow_vis\n",
    "from scipy.stats import zscore\n",
    "from scipy import optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f144012-7737-482c-b40c-e4b5d7b8957f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define sample range (first and last frames)\n",
    "sample_frames = (193100, 194100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c195e2-7896-4ff5-853e-f7cb1a61f0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define file paths and load DLC file (will not load behaviour MP4 into memory!)\n",
    "facevidpath = '/Users/Oliver/Google Drive/Imaging_Results/171_172_173_174/Basler acA780-75gm (22611477)_20190220_105916470.mp4'\n",
    "dlc_facepath = '/Users/Oliver/Google Drive/Imaging_Results/171_172_173_174/Basler acA780-75gm (22611477)_20190220_105916470DeepCut_resnet50_MouseFaceAug21shuffle1_1030000.h5'\n",
    "dlc_face = pd.read_hdf(dlc_facepath, mode='r').loc[sample_frames[0]:sample_frames[1]]\n",
    "dlc_face.columns = dlc_face.columns.droplevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ec08d-6772-4c18-a7eb-d77ef6650902",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be7922-7f23-4b6d-85da-0b957d5ddf5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# play video with OpenCV in new window (STOP WITH 'Q')\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,sample_frames[0])\n",
    "i=0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    frame_width  = int(cap.get(3))   # float `width`\n",
    "    frame_height = int(cap.get(4))  # float `height\n",
    "    fps = 1/cap.get(2)\n",
    "    if ret == True:\n",
    "        frame = cv2.putText(frame, str(i+sample_frames[0]), org=(50,50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow('frame',frame)\n",
    "        i+=1\n",
    "    else:\n",
    "        break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e76f0-f92b-4ac2-a849-cae5e45529b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show DLC dataframe\n",
    "dlc_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f8598-c14a-41f6-8b48-90253025d1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# play video with OpenCV with DLC points added, in new window (STOP WITH 'Q')\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,sample_frames[0])\n",
    "i=sample_frames[0]\n",
    "while(cap.isOpened()):\n",
    "    facetime = dlc_face.loc[i]\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        # add DLC markers:\n",
    "        for j, marker in enumerate(facetime.index.get_level_values(level=0).unique().tolist()):\n",
    "            image = cv2.circle(frame, (int(facetime[marker].x), int(facetime[marker].y)), radius=10, \n",
    "                               color=[k*255 for k in cm.jet(round(255/12*j))[:3]], thickness=-1)\n",
    "        image = cv2.putText(image, str(i), org=(50,50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow('frame',image)\n",
    "    else:\n",
    "        break\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')) | (i>sample_frames[1]-2):\n",
    "        break\n",
    "    i+=1\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a377146-6957-4ee3-af87-bd0373ddb654",
   "metadata": {},
   "source": [
    "## Estimate pupil diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b81cf-e5eb-44a3-a61c-96ea9fd21a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up DLC data by thresholding likelihood:\n",
    "like_thresh = 0.9  # likelihood threshold points need to pass\n",
    "dlc_face_clean = dlc_face.copy()\n",
    "dlc_face_clean.loc(axis=1)[:, [\"x\", \"y\"]] = (np.tile([dlc_face_clean.loc(axis=1)[:, \"likelihood\"].to_numpy()], 2)[0] > like_thresh) * dlc_face_clean.loc(axis=1)[:, [\"x\", \"y\"]]\n",
    "dlc_face_clean[dlc_face_clean==0] = np.nan  # set markers with low likelihood to NaN\n",
    "dlc_face_clean = dlc_face_clean.interpolate()  # interpolate linearly across NaNs\n",
    "dlc_face_clean = dlc_face_clean.fillna(method='bfill').fillna(method='ffill')  # back- and forward-fill NaNs at end and beginning\n",
    "dlc_face_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5ac56-3e3c-46d1-b42d-5294c99eb775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# determine pupil diameter and plot with OpenCV\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,sample_frames[0])\n",
    "i=sample_frames[0]\n",
    "while(cap.isOpened()):\n",
    "    facetime = dlc_face_clean.loc[i]\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        # determine pupil and plot:\n",
    "        pupilpoints = np.float32(facetime.loc[['pupil1', 'pupil2', 'pupil3', 'pupil4', 'pupil5', 'pupil6'], ['x', 'y']].values.reshape(6,2))\n",
    "        pupil_circle = np.asarray(cv2.minEnclosingCircle(pupilpoints), dtype='object')\n",
    "        image = cv2.circle(frame, [int(k) for k in pupil_circle[0]], radius=int(pupil_circle[1]), color=[255, 255, 255], thickness=2)\n",
    "        # add DLC markers:\n",
    "        for j, marker in enumerate(facetime.index.get_level_values(level=0).unique().tolist()):\n",
    "            image = cv2.circle(frame, (int(facetime[marker].x), int(facetime[marker].y)), radius=5, \n",
    "                               color=[k*255 for k in cm.jet(round(255/12*j))[:3]], thickness=-1)\n",
    "        image = cv2.putText(image, str(i), org=(50,50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow('frame',image)\n",
    "    else:\n",
    "        break\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('q')) | (i>sample_frames[1]-2):\n",
    "        break\n",
    "    i+=1\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54a8dd-2f25-4087-b910-15529702043c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract pupil diameter\n",
    "pupil_diam = pd.Series(index=range(sample_size), dtype='object')\n",
    "for i in tqdm(range(sample_size)):\n",
    "    facetime = dlc_face_clean.iloc[i]\n",
    "    pupilpoints = np.float32(facetime.loc[['pupil1', 'pupil2', 'pupil3', 'pupil4', 'pupil5', 'pupil6'], ['x', 'y']].values.reshape(6,2))\n",
    "    pupil_circle = np.asarray(cv2.minEnclosingCircle(pupilpoints), dtype='object')\n",
    "    pupil_diam.loc[i] = pupil_circle[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10681e-f65d-4b34-b282-d86cd013708b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hanning filter\n",
    "def smooth(x, window_len=11, window='hanning'):\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-2:-window_len-1:-1]]\n",
    "    w = eval('np.'+window+'(window_len)')\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y\n",
    "\n",
    "# smoothen signal\n",
    "smooth_window = 75\n",
    "pupil_diam_smooth = pd.Series(smooth(pupil_diam, window_len=smooth_window)).shift(periods=-int(smooth_window / 2))[:sample_size].astype(float)\n",
    "\n",
    "# plot raw and smoothed pupil diameter\n",
    "pupil_diam_df = pd.DataFrame({'raw': pupil_diam, 'filtered': pupil_diam_smooth})\n",
    "pupil_diam_df.astype(float).plot_bokeh(xlabel='Frame', ylabel='Pupil diametre [au]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49c3f4-a459-496e-9e2c-7994ba17e915",
   "metadata": {},
   "source": [
    "## Estimate face regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb140325-cb21-4677-99e8-6c36064049d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find face regions based on median locations\n",
    "face_anchor = dlc_face_clean.loc(axis=1)[['nosetip', 'forehead', 'mouthtip', 'chin', 'tearduct', 'eyelid2'], ['x', 'y']].median()\n",
    "whiskercentre = np.array(cv2.minEnclosingCircle(np.array(np.vstack([face_anchor.nosetip, face_anchor.mouthtip, face_anchor.tearduct]), dtype=np.float32))[0])\n",
    "whiskercentre = tuple(np.round(whiskercentre).astype(int) + [-10, 70])\n",
    "nosecentre = tuple(np.round(face_anchor.nosetip).astype(int) + [50, 0])\n",
    "mouthcentre = tuple(np.round(face_anchor.mouthtip + (face_anchor.chin - face_anchor.mouthtip)/3).astype(int))\n",
    "mouthangle = math.degrees(math.atan2((face_anchor.chin[1] - face_anchor.mouthtip[1]), (face_anchor.chin[0] - face_anchor.mouthtip[0])))\n",
    "cheek_centre = tuple(np.round(face_anchor.eyelid2 + (face_anchor.chin - face_anchor.eyelid2)/2).astype(int))\n",
    "\n",
    "# plot median points\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "firstframe = np.array(cap.read()[1][:, :, 0], dtype = np.uint8)\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "for anchor in face_anchor.index.get_level_values(0).unique().tolist():\n",
    "    ax.scatter(face_anchor[anchor]['x'], face_anchor[anchor]['y'], s=300)\n",
    "    \n",
    "# plot whisker pad\n",
    "ax.plot([face_anchor.nosetip[0], face_anchor.tearduct[0]], [face_anchor.nosetip[1], face_anchor.tearduct[1]], color='w', lw=3)\n",
    "ax.plot([face_anchor.nosetip[0], face_anchor.mouthtip[0]], [face_anchor.nosetip[1], face_anchor.mouthtip[1]], color='w', lw=3)\n",
    "ax.plot([face_anchor.mouthtip[0], face_anchor.tearduct[0]], [face_anchor.mouthtip[1], face_anchor.tearduct[1]], color='w', lw=3)\n",
    "ax.scatter(whiskercentre[0], whiskercentre[1], s=300, c='w')\n",
    "whiskers = cv2.circle(firstframe, whiskercentre, 100, color=(255, 0, 0), thickness=5)\n",
    "\n",
    "# plot nose region\n",
    "ax.scatter(nosecentre[0], nosecentre[1], s=300, c='w')\n",
    "nose = cv2.ellipse(firstframe, nosecentre, (70, 50), -60.0, 0.0, 360.0, (255, 0, 0), 5)\n",
    "\n",
    "# plot mouth region\n",
    "ax.plot([face_anchor.mouthtip[0], face_anchor.chin[0]], [face_anchor.mouthtip[1], face_anchor.chin[1]], color='w', lw=3)\n",
    "ax.scatter(mouthcentre[0], mouthcentre[1], s=300, c='w')\n",
    "mouth = cv2.ellipse(firstframe, mouthcentre, (160, 60), mouthangle, 0.0, 360.0, (255, 0, 0), 5)\n",
    "\n",
    "# plot cheek region\n",
    "ax.plot([face_anchor.eyelid2[0], face_anchor.chin[0]], [face_anchor.eyelid2[1], face_anchor.chin[1]], color='w', lw=3)\n",
    "ax.scatter(cheek_centre[0], cheek_centre[1], s=300, c='w')\n",
    "cheek = cv2.ellipse(firstframe, cheek_centre, (180, 100), 0.0, 0.0, 360.0, (255, 0, 0), 5)\n",
    "\n",
    "ax.imshow(firstframe, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c708e62-5531-49a3-9b76-6c72556f4618",
   "metadata": {},
   "source": [
    "## Calculate motion energy within face regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312a006-7593-46e5-af5a-9d56b04d6efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show motion energy with OpenCV in new window (STOP WITH 'Q')\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,sample_frames[0])\n",
    "ret, current_frame = cap.read()\n",
    "previous_frame = current_frame\n",
    "i=sample_frames[0]\n",
    "while(cap.isOpened()):\n",
    "    if ret == True:\n",
    "        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_diff = cv2.applyColorMap(cv2.absdiff(current_frame_gray, previous_frame_gray), cv2.COLORMAP_VIRIDIS)\n",
    "        whiskers = cv2.circle(frame_diff, whiskercentre, 100, color=(255, 0, 0), thickness=2)\n",
    "        nose = cv2.ellipse(frame_diff, nosecentre, (70, 50), -60.0, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        mouth = cv2.ellipse(frame_diff, mouthcentre, (160, 60), mouthangle, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        cheek = cv2.ellipse(frame_diff, cheek_centre, (180, 100), 0.0, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        frame_diff = cv2.putText(frame_diff, str(i), org=(50,50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow('frame', frame_diff)\n",
    "        previous_frame = current_frame.copy()\n",
    "        ret, current_frame = cap.read()\n",
    "        i+=1\n",
    "    else:\n",
    "        break\n",
    "    if (i>sample_frames[1]-2):  # at end of samples, loop from beginning\n",
    "        i=sample_frames[0]\n",
    "        cap.set(1,sample_frames[0])\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d06cdd-52a0-450f-9ad6-0d29499252a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define face region masks\n",
    "canvas = np.zeros([frame_height, frame_width])\n",
    "whiskermask = cv2.circle(canvas.copy(), whiskercentre, 100, color=(1, 0, 0), thickness=-1).astype(bool)\n",
    "nosemask = cv2.ellipse(canvas.copy(), nosecentre, (70, 50), -60.0, 0.0, 360.0, (1, 0, 0), -1).astype(bool)\n",
    "mouthmask = cv2.ellipse(canvas.copy(), mouthcentre, (160, 60), mouthangle, 0.0, 360.0, (1, 0, 0), -1).astype(bool)\n",
    "cheekmask = cv2.ellipse(canvas.copy(), cheek_centre, (180, 100), 0.0, 0.0, 360.0, (1, 0, 0), -1).astype(bool)\n",
    "masks = np.array([nosemask, whiskermask, mouthmask, cheekmask]).astype('float32')\n",
    "masks[masks==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470f17d-6fdf-4104-95c7-1ff98a7f0500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate motion energy\n",
    "mask_me = np.empty((sample_size, masks.shape[0]))\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,sample_frames[0])\n",
    "ret, current_frame = cap.read()\n",
    "previous_frame = current_frame\n",
    "i=0\n",
    "with tqdm(total=sample_size) as pbar:\n",
    "    while(cap.isOpened()):\n",
    "        if ret == True:\n",
    "            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "            previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "            frame_diff = cv2.absdiff(current_frame_gray, previous_frame_gray)\n",
    "            for j, mask in enumerate(masks):\n",
    "                mask_me[i, j] = np.nanmean(frame_diff * mask)\n",
    "            pbar.update(1)\n",
    "            previous_frame = current_frame.copy()\n",
    "            ret, current_frame = cap.read()\n",
    "            i+=1\n",
    "        else:\n",
    "            break\n",
    "        if i>sample_size-2:\n",
    "            break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754d878-c62b-4d9a-82bb-a12d41777ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot raw and filtered motion energy\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "motion_energy = pd.DataFrame(data=mask_me, columns=['nose', 'whiskers', 'mouth', 'cheek'], index=range(sample_frames[0], sample_frames[1]))\n",
    "for i, face_region in enumerate(motion_energy.columns):\n",
    "    ax.plot(motion_energy[face_region]-i*15, lw=1)\n",
    "    ax.plot(pd.Series(smooth(motion_energy[face_region]-i*15, window_len=75)[:sample_size], index=range(sample_frames[0], sample_frames[1])).shift(periods=-int(75/2)), lw=3)\n",
    "    ax.set_xlabel('Frame')\n",
    "    ax.set(ylabel=None)  # remove the y-axis label\n",
    "    ax.tick_params(left=False)  # remove the ticks\n",
    "    ax.set_yticks([0, -15, -30, -45])\n",
    "    ax.set(yticklabels=motion_energy.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14988e5-ff4a-4c5a-8861-e4f6e0bba470",
   "metadata": {},
   "source": [
    "## Calculate optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c371c-7c7b-40e1-8ca0-a78e81723730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# focus on subset of frames because processing is slower (on CPU)\n",
    "flow_sample_frames = (193470, 193670)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88622d77-e6d6-46f2-b9f5-6316f96f6ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show optical flow with OpenCV in new window (STOP WITH 'Q')\n",
    "# see also https://www.geeksforgeeks.org/python-opencv-dense-optical-flow/\n",
    "use_flow_vis = True  # whether to use flow_vis package to visualise optical flow\n",
    "blend_gray_optflow = 0.2  # factor for blending in gray frame with optical flow color output video\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,flow_sample_frames[0])\n",
    "ret, current_frame = cap.read()\n",
    "previous_frame = current_frame\n",
    "mask = np.zeros_like(current_frame)\n",
    "mask[..., 1] = 255\n",
    "i=flow_sample_frames[0]\n",
    "while(cap.isOpened()):\n",
    "    if ret == True:\n",
    "        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "        previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        if use_flow_vis:\n",
    "            rgb = flow_vis.flow_to_color(flow, convert_to_bgr=True)\n",
    "        else:\n",
    "            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            mask[..., 0] = angle * 180 / np.pi / 2\n",
    "            mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "        if blend_gray_optflow:  # blend in gray frame\n",
    "            rgb = cv2.addWeighted(current_frame, blend_gray_optflow, rgb, 1-blend_gray_optflow, 0)\n",
    "        whiskers = cv2.circle(rgb, whiskercentre, 100, color=(255, 0, 0), thickness=2)\n",
    "        nose = cv2.ellipse(rgb, nosecentre, (70, 50), -60.0, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        mouth = cv2.ellipse(rgb, mouthcentre, (160, 60), mouthangle, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        cheek = cv2.ellipse(rgb, cheek_centre, (180, 100), 0.0, 0.0, 360.0, (255, 0, 0), 2)\n",
    "        rgb = cv2.putText(rgb, str(i), org=(50,50), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow(\"dense optical flow\", rgb)\n",
    "        previous_frame = current_frame.copy()\n",
    "        ret, current_frame = cap.read()\n",
    "        i+=1\n",
    "    else:\n",
    "        break\n",
    "    if (i>flow_sample_frames[1]-2):  # at end of samples, loop from beginning\n",
    "        i=flow_sample_frames[0]\n",
    "        cap.set(1,flow_sample_frames[0])\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742f4ef-1a12-485b-a1cc-3d9bccd39a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate average optical flow magnitude/angle per face region, this is slow on CPU!\n",
    "flow_sample_size = flow_sample_frames[1] - flow_sample_frames[0]  # because this takes longer, take a smaller sample\n",
    "mask_flow_mag = np.empty((flow_sample_size, masks.shape[0]))\n",
    "mask_flow_ang = np.empty((flow_sample_size, masks.shape[0]))\n",
    "cap = cv2.VideoCapture(facevidpath)\n",
    "cap.set(1,flow_sample_frames[0])\n",
    "ret, current_frame = cap.read()\n",
    "previous_frame = current_frame\n",
    "i=0\n",
    "with tqdm(total=flow_sample_size) as pbar:\n",
    "    while(cap.isOpened()):\n",
    "        if ret == True:\n",
    "            current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "            previous_frame_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)\n",
    "            flow = cv2.calcOpticalFlowFarneback(previous_frame_gray, current_frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            for j, mask in enumerate(masks):\n",
    "                mask_flow_mag[i, j] = np.nanmean(magnitude * mask)\n",
    "                mask_flow_ang[i, j] = np.nanmean(angle * mask)\n",
    "            pbar.update(1)\n",
    "            previous_frame = current_frame.copy()\n",
    "            ret, current_frame = cap.read()\n",
    "            i+=1\n",
    "        else:\n",
    "            break\n",
    "        if i>flow_sample_size-2:\n",
    "            break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c10456-0dc2-4fb0-9874-00ec544e9132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot optical flow\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "ang = pd.DataFrame(data=mask_flow_ang, columns=['nose', 'whiskers', 'mouth', 'cheek'], index=range(flow_sample_frames[0], flow_sample_frames[1]))\n",
    "mag = pd.DataFrame(data=mask_flow_mag, columns=['nose', 'whiskers', 'mouth', 'cheek'], index=range(flow_sample_frames[0], flow_sample_frames[1]))\n",
    "for i, face_region in enumerate(ang.columns):\n",
    "    ax[0].plot(ang[face_region]-i*4)\n",
    "    ax[0].set_xlabel('Frame')\n",
    "    ax[0].set_ylabel('Optical flow angle')\n",
    "    ax[0].set_yticks([2, -2, -6, -10])\n",
    "    ax[0].set(yticklabels=ang.columns)  \n",
    "    \n",
    "    ax[1].plot(mag[face_region]-i*8)\n",
    "    ax[1].set_xlabel('Frame')\n",
    "    ax[1].set_ylabel('Optical flow magnitude')\n",
    "    ax[1].set_yticks([4, -4, -12, -20])\n",
    "    ax[1].set(yticklabels=mag.columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4aebe2-8163-43d7-aeb0-ab682d0ebcbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ang.whiskers.plot_bokeh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a9a1d-3032-442f-bbb0-af4fb4ee369a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sine fitting written by unsym: https://stackoverflow.com/a/42322656\n",
    "def fit_sin(yy):\n",
    "    '''Fit sin to the input time sequence, and return fitting parameters \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\"'''\n",
    "    tt = np.array(range(len(yy)))\n",
    "    yy = np.array(yy)\n",
    "    ff = np.fft.fftfreq(len(tt), (tt[1] - tt[0]))  # assume uniform spacing\n",
    "    Fyy = abs(np.fft.fft(yy))\n",
    "    guess_freq = abs(ff[np.argmax(Fyy[1:]) + 1])  # excluding the zero frequency \"peak\", which is related to offset\n",
    "    guess_amp = np.std(yy) * 2. ** 0.5\n",
    "    guess_offset = np.mean(yy)\n",
    "    guess = np.array([guess_amp, 2. * np.pi * guess_freq, 0., guess_offset])\n",
    "\n",
    "    def sinfunc(t, A, w, p, c):\n",
    "        return A * np.sin(w * t + p) + c\n",
    "\n",
    "    try:\n",
    "        popt, pcov = optimize.curve_fit(sinfunc, tt, yy, p0=guess)\n",
    "        A, w, p, c = popt\n",
    "        f = w / (2. * np.pi)\n",
    "        fitfunc = lambda t: A * np.sin(w * t + p) + c\n",
    "        return {\"amp\": A, \"omega\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1. / f, \"fitfunc\": fitfunc,\n",
    "                \"maxcov\": np.max(pcov), \"rawres\": (guess, popt, pcov)}\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def fit_sin_w(yy):  # omega\n",
    "    '''Fit sin to the input time sequence, and return fitting parameters \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\"'''\n",
    "    tt = np.array(range(len(yy)))\n",
    "    yy = np.array(yy)\n",
    "    ff = np.fft.fftfreq(len(tt), (tt[1] - tt[0]))  # assume uniform spacing\n",
    "    Fyy = abs(np.fft.fft(yy))\n",
    "    guess_freq = abs(ff[np.argmax(Fyy[1:]) + 1])  # excluding the zero frequency \"peak\", which is related to offset\n",
    "    guess_amp = np.std(yy) * 2. ** 0.5\n",
    "    guess_offset = np.mean(yy)\n",
    "    guess = np.array([guess_amp, 2. * np.pi * guess_freq, 0., guess_offset])\n",
    "\n",
    "    def sinfunc(t, A, w, p, c):\n",
    "        return A * np.sin(w * t + p) + c\n",
    "    try:\n",
    "        popt, pcov = optimize.curve_fit(sinfunc, tt, yy, p0=guess)\n",
    "        A, w, p, c = popt\n",
    "        return w\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def fit_sin_error(yy):  # mean SD error\n",
    "    '''Fit sin to the input time sequence, and return fitting parameters \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\"'''\n",
    "    tt = np.array(range(len(yy)))\n",
    "    yy = np.array(yy)\n",
    "    ff = np.fft.fftfreq(len(tt), (tt[1] - tt[0]))  # assume uniform spacing\n",
    "    Fyy = abs(np.fft.fft(yy))\n",
    "    guess_freq = abs(ff[np.argmax(Fyy[1:]) + 1])  # excluding the zero frequency \"peak\", which is related to offset\n",
    "    guess_amp = np.std(yy) * 2. ** 0.5\n",
    "    guess_offset = np.mean(yy)\n",
    "    guess = np.array([guess_amp, 2. * np.pi * guess_freq, 0., guess_offset])\n",
    "    def sinfunc(t, A, w, p, c):\n",
    "        return A * np.sin(w * t + p) + c\n",
    "    try:\n",
    "        popt, pcov = optimize.curve_fit(sinfunc, tt, yy, p0=guess)\n",
    "        A, w, p, c = popt\n",
    "        return np.mean(np.sqrt(np.diag(pcov)))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab328-26c7-4ca0-bcf8-c8fee9e4be05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit sine and extract frequency\n",
    "def freq_analysis(x, fps=75, rollwin=50, min_periods=30, conf=0.5):\n",
    "    xz = pd.Series(zscore(x.values))\n",
    "    print('Fitting sine...')\n",
    "    w = xz.rolling(int(rollwin), center=True, min_periods=min_periods).apply(fit_sin_w)\n",
    "    error = xz.rolling(int(rollwin), center=True, min_periods=min_periods).apply(fit_sin_error)\n",
    "    w_clean = w.copy()\n",
    "    w_clean[w_clean.diff().abs()>(.75/fps)] = np.nan\n",
    "    w_clean[error>(fps/100)] = np.nan\n",
    "    w_clean[w_clean<0] = np.nan\n",
    "    w_clean = w_clean.interpolate(method='polynomial', order=3, limit=int(fps*2))\n",
    "    w_smooth = w_clean.rolling(int(rollwin/3), center=True, min_periods=int(min_periods/3)).mean(window='gaussian')\n",
    "    f_smooth = w_smooth/(2.*np.pi)*fps\n",
    "    return f_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa0c97-a53b-4f37-b544-d3f13e4d71de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Illustrate sinusoid fit on a patch of whisking optical flow angle\n",
    "res = fit_sin(ang.whiskers.loc[193580:193630])\n",
    "plt.plot(ang.whiskers.loc[193580:193630].values, label='raw signal (optical flow angle)')\n",
    "plt.plot(res['fitfunc'](range(50)), label='sinusoid fit')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dd5d2-a4c7-4deb-80b7-5fb2e54735e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply sine fit to all optical flow angle data\n",
    "face_freq = ang.apply(freq_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ffc6f-0070-4230-9090-a2ebc03bc108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "face_freq.index = pd.Index(range(flow_sample_frames[0], flow_sample_frames[1]))\n",
    "face_freq.plot_bokeh(ylabel='Frequency [Hz]', xlabel='Frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528e60e-b503-4e03-afc2-3ac71d89daf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
