{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavior Analysis using DeepLabCut:**\n",
    "This notebook will illustrate how to:\n",
    "\n",
    "* Create a new DeepLabCut project\n",
    "* Sample videos to obtain frames\n",
    "* Label the bodyparts in the frames\n",
    "* Creating a training dataset\n",
    "* Training a network\n",
    "* Evaluating the network\n",
    "* Analyzing videos\n",
    "\n",
    "\n",
    "Nath, Mathis et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019. Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
    "\n",
    "**All the necessary toolkits are already present in the LINdoscope environment. Please follow the instructions for each cell and enjoy !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a new project :\n",
    "You can use this function to create a new project with sub-directories and a basic configuration file in the user defined directory. Otherwise the project is created in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='lindoscope_analysis' # Enter the name of your experiment Task\n",
    "experimenter='LINdoscope' # Enter the name of the experimenter\n",
    "working_directory= '/home/pbauer/DLC' # Enter the path to the working directory where you want to\n",
    "                                                                     # create the project\n",
    "videos=['/home/pbauer/DATA/video-1.mp4'] # Enter the paths of your videos \n",
    "                                                                                               # where you want to grab frames from\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,videos,working_directory,copy_videos=True, videotype=\".mp4\", multianimal=False) \n",
    "# The function returns the path, where your project is\n",
    "# Check the folder to see if the config file has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_file='/home/pbauer/DLC/lindoscope_analysis-LINdoscope-2021-08-30/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit the config.yaml files\n",
    "**The parameters that you can adjust are:**\n",
    "\n",
    "* Number of frames to extract for a video (change the 'numframes2pick' parameter)\n",
    "* Labels for the bodypart markers (list the bodyparts you would like to label. eg. tailtip, eartipleft etc.)\n",
    "* Size of the markers (change the 'dotsize' parameter)\n",
    "* p cutoff value (you want this value to be high, this ensures the network plots the points it is confident about)\n",
    "* Croping of the video (you can also crop the video by setting the 'crop' parameter to 'True' and then adjusting the x1,x2,y1 and y2 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract frames from the video :\n",
    "You can use this function to extract frames from the videos to label the bodyparts. A succesfulfeature detector requires a diverse selection of frames. The frames can be extracted using uniform sampling, k-means sampling or manual selection. This is a good point to also check how the cropped frames look, if you have chosen to crop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "extracted_frame = deeplabcut.extract_frames(path_config_file,  mode=\"automatic\", algo=\"kmeans\") # Automatic k-means frame\n",
    "# extraction. Set algo='uniform' to extract the frames uniformly\n",
    "extracted_frame = deeplabcut.extract_frames(path_config_file,  mode=\"manual\") # Manual extraction of frames\n",
    "plt.imshow(extracted_frame) # Plot the frame to make sure the cropping parameters work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Labelling the frames :\n",
    "Now you can label the extracted frames with the bodypart labels you defined in the config file. The labeled images will be saved in the project directory in the sub-directory 'labeled-data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "\n",
    "deeplabcut.label_frames(path_config_file)\n",
    "# At this point you can check the labels since this is the most cucial step for creating a training dataset.\n",
    "\n",
    "deeplabcut.check_labels(path-config_file)\n",
    "# You can now check the frames with your labels in the 'check-labels' subdirectory\n",
    "# You can also adjust the labels by relaunching the labeling GUI and move the labels around and click save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a training dataset :\n",
    "You can create a training dataset using the function provided below. This geenrates the training information that the network will use. You can adjust the training and test fraction in the config.yaml file by changing the 'TrainingFraction' parameter. You can also create multiple shuffles of the data to benchmark performance, the default is set to 1. There are also several networks that you can pick from, the default network used here is the resnet-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start training (we dont do it now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze your videos :\n",
    "If you ae happy with the training of your network, you can now start analyzing your videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the videos you want to analyze:\n",
    "path_config_file='/home/lindoscopeuserXX/Restrained_VGluT2_OpenFieldMar29'\n",
    "dir_out='/home/lindoscopeuserXX/yourDLCfolder'\n",
    "analyze_videopath = ['/home/lindoscopeuserXX/yvideo-1.mp4']\n",
    "#gpuid=1\n",
    "\n",
    "deeplabcut.analyze_videos(path_config_file, analyze_videopath, videotype='.mp4', destfolder=dir_out, gputouse=id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating labeled videos :\n",
    "Now you can visualize the fruits of your labour, the following function can be used to create .mp4 videos with the labels predicted by the network. The videos are saved in the same directory as the original videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file)\n",
    "\n",
    "# Plot the trajectories of the analyzed vidoes\n",
    "%matplotlib notebook\n",
    "deeplabcut.plot_trajectories(path_config_file, analyz_videopath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
